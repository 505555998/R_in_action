headers$value()
h = basicHeaderGatherer()
txtt=getURL("http://www.dataguru.cn/",headerfunction = h$update)
names(h$value())
names(headers$value())
h$value()
curl = getCurlHandle()
d=getURL("http://www.dataguru.cn/", curl = curl)
getCurlInfo(curl)$response.code
getCurlInfo(curl)
url = getURL("http://rfunction.com/code/1202/")
temp<- getBinaryURL(url)
wp = getURL("http://www.bioguo.org/AnimalTFDB/BrowseAllTF.php?spe=Mus_musculus")
doc <- htmlParse(wp, asText = TRUE)
library(XML)
doc <- htmlParse(wp, asText = TRUE)
tables <- readHTMLTable(doc)
tables
head(tables)
str(tables)
tables$table1
head(tables$table1
)
head(tables$table1)
dizhen = getURL("http://data.earthquake.cn/datashare/datashare_more_quickdata_new.jsp")
dizhen_data <- getURL(dizhen)
doc <-htmlParse(wp, asText = TRUE)
tables <- readHTMLTable(doc,header=F)
head(tables)
head(tables$table1)
dizhen_doc <-htmlParse(dizhen, asText = TRUE)
dizhen_tables <- readHTMLTable(dizhen_doc,header=F)
head(dizhen_tables)
install.packages("RExcelInstaller", "rcom", "rsproxy")
require(devtools)
install.packages(devtools)
install.packages(“httr”)
install.packages("httr")
install.packages("devtools")
install_github("ramnathv/rCharts")
install_github("rcharts/ramnathv")
require(devtools)
find_rtools()
install.packages(c("abind", "adabag", "AER", "amap", "arules", "bayesm", "bdsmatrix", "BH", "boot", "bootstrap", "BradleyTerry2", "BRugs", "cairoDevice", "car", "caret", "caTools", "chron", "class", "classInt", "cluster", "coda", "colorspace", "corpcor", "corrgram", "coxme", "DCluster", "deldir", "DEoptimR", "deSolve", "diagram", "digest", "doBy", "dplyr", "dse", "e1071", "Ecdat", "Ecfun", "effects", "emplik", "evaluate", "fBasics", "fda", "fields", "forecast", "foreign", "formatR", "Formula", "gam", "gdata", "gee", "geoR", "geosphere", "GGally", "ggm", "ggmap", "ggplot2", "glmnet", "goftest", "GPArotation", "gplots", "gss", "gstat", "gtools", "gWidgetsRGtk2", "HH", "highr", "Hmisc", "htmltools", "httpuv", "igraph", "intervals", "ipred", "JM", "JMbayes", "kernlab", "KernSmooth", "kknn", "knitr", "kohonen", "labeling", "lava", "lme4", "lmtest", "manipulate", "mapdata", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "mboost", "mfp", "mime", "minqa", "mnormt", "MSBVAR", "multcomp", "mutoss", "mvoutlier", "nlme", "NLP", "nnet", "numDeriv", "party", "pcaPP", "pgirmess", "plotrix", "plspm", "plyr", "polyclip", "prodlim", "pscl", "psych", "pwr", "quantmod", "R.matlab", "R.methodsS3", "R.oo", "R.utils", "R2WinBUGS", "R6", "RandomFields", "randomForest", "RANN", "raster", "Rcmdr", "RColorBrewer", "Rcpp", "RcppArmadillo", "RcppEigen", "RCurl", "reshape2", "rgdal", "rgeos", "rgl", "RgoogleMaps", "rjson", "rmarkdown", "rminer", "robCompositions", "robustbase", "ROCR", "RODBC", "rpart", "rpart.plot", "rrcov", "sandwich", "scales", "sem", "seriation", "setRNG", "shape", "shiny", "sp", "spacetime", "spam", "SparseM", "spatial", "spatstat", "spBayes", "spdep", "sphet", "splancs", "splm", "stabledist", "statmod", "stringr", "strucchange", "survival", "systemfit", "tcltk2", "tfplot", "tframe", "TH.data", "timeDate", "timeSeries", "tm", "tsDyn", "tseries", "TSP", "TTR", "vcd", "VIM", "xlsxjars", "XML", "xtable", "zoo"))
install.packages(c("abind", "adabag", "AER", "amap", "arules",
install.packages(c("abind", "adabag", "AER", "amap", "arules", "bayesm", "bdsmatrix", "BH", "boot", "bootstrap", "BradleyTerry2", "BRugs", "cairoDevice", "car", "caret", "caTools", "chron", "class", "classInt", "cluster", "coda", "colorspace", "corpcor", "corrgram", "coxme", "DCluster", "deldir", "DEoptimR", "deSolve", "diagram", "digest", "doBy", "dplyr", "dse", "e1071", "Ecdat", "Ecfun", "effects", "emplik", "evaluate", "fBasics", "fda", "fields", "forecast", "foreign", "formatR", "Formula", "gam", "gdata", "gee", "geoR", "geosphere", "GGally", "ggm", "ggmap", "ggplot2", "glmnet", "goftest", "GPArotation", "gplots", "gss", "gstat", "gtools", "gWidgetsRGtk2", "HH", "highr", "Hmisc", "htmltools", "httpuv", "igraph", "intervals", "ipred", "JM", "JMbayes", "kernlab", "KernSmooth", "kknn", "knitr", "kohonen", "labeling", "lava", "lme4", "lmtest", "manipulate", "mapdata", "mapproj", "maps", "maptools", "markdown", "MASS", "Matrix", "mboost", "mfp", "mime", "minqa", "mnormt", "MSBVAR", "multcomp", "mutoss", "mvoutlier", "nlme", "NLP", "nnet", "numDeriv", "party", "pcaPP", "pgirmess", "plotrix", "plspm", "plyr", "polyclip", "prodlim", "pscl", "psych", "pwr", "quantmod", "R.matlab", "R.methodsS3", "R.oo", "R.utils", "R2WinBUGS", "R6", "RandomFields", "randomForest", "RANN", "raster", "Rcmdr", "RColorBrewer", "Rcpp", "RcppArmadillo", "RcppEigen", "RCurl", "reshape2", "rgdal", "rgeos", "rgl", "RgoogleMaps", "rjson", "rmarkdown", "rminer", "robCompositions", "robustbase", "ROCR", "RODBC", "rpart", "rpart.plot", "rrcov", "sandwich", "scales", "sem", "seriation", "setRNG", "shape", "shiny", "sp", "spacetime", "spam", "SparseM", "spatial", "spatstat", "spBayes", "spdep", "sphet", "splancs", "splm", "stabledist", "statmod", "stringr", "strucchange", "survival", "systemfit", "tcltk2", "tfplot", "tframe", "TH.data", "timeDate", "timeSeries", "tm", "tsDyn", "tseries", "TSP", "TTR", "vcd", "VIM", "xlsxjars", "XML", "xtable", "zoo"))
install.packages(c("abind", "adabag", "AER", "amap", "arules",
library("abind", lib.loc="E:/Program Files/R/R-3.2.1/library")
install.packages("dse")
install.packages("GPArotation")
install.packages(c("timeDate", "timeSeries", "tseries"))
library("dplyr", lib.loc="E:/Program Files/R/R-3.2.1/library")
lbrary(dplyr)
library(dplyr)
library("plyr", lib.loc="E:/Program Files/R/R-3.2.1/library")
xx<- array(1:24,c(3,4,2))
a<-array(1:21,c(3,7))
require(plyr)
library(plyr)
aaply(.data=a, .margins=1, .fun=mean)
a
xx
aaply(.data=a,  2, mean,  process="text")
names=c("john","mary","Alice","Peter","ROger","Phyillis")
age=c(13,15,14,13,14,13)
sex=c("M","F","F","M","M","F")
data=data.frame(names,age,sex)
aaply(.data=xx, .margins=1, .fun=mean)
aaply(.data=xx,  .margins = 2, .fun=mean,  process="text")
amean=function(data)
{
agemean=mean(data[,2])
return(agemean)
}
daply(data,.(sex,age),amean)
data
.parseISO8601('2000')
library(xts)
.parseISO8601('2000')
x <- timeBasedSeq('2010-01-01/2010-01-02 12:00')
x <- xts(1:length(x), x)
head(x)
indexClass(x)
indexFormat(x) <- "%Y-%b-%d %H:%M:%OS3"
head(x)
indexFormat(x) <- "%Y-%b-%d %H:%M:%OS3"
head(x)
x <- Sys.time() + 1:30
x
data(sample_matrix)
to.period(sample_matrix)
class(to.period(sample_matrix))
samplexts <- as.xts(sample_matrix)
to.period(samplexts)
class(to.period(samplexts))
endpoints(sample_matrix)
endpoints(sample_matrix, 'days',k=7)
endpoints(sample_matrix, 'weeks')
endpoints(sample_matrix, 'months')
(x <- xts(4:10, Sys.Date()+4:10))
(y <- xts(1:6, Sys.Date()+1:6))
merge(x,y)
merge(x,y, join='inner')
merge(x,y, join='left')
data(sample_matrix)
x <- as.xts(sample_matrix)
x
split(x)[[1]]
split(x)[[2]]
x <- xts(1:10, Sys.Date()+1:10)
x[c(1,2,5,9,10)] <- NA
x
na.locf(x)
na.locf(x, fromLast=TRUE)
xts.ts <- xts(rnorm(231),as.Date(13514:13744,origin="1970-01-01"))
start(xts.ts)
end(xts.ts)
zoo.data <- zoo(rnorm(31)+10,as.Date(13514:13744,origin="1970-01-01"))
ep <- endpoints(zoo.data,'weeks')
ep
timeBased(Sys.time())
timeBased(Sys.Date())
timeBased(200701)
timeBasedSeq('20080101 0830',length=100)
x <- xts(1:5, Sys.Date()+1:5)
x
lag(x)
library(RCurl)
headers = basicTextGatherer()
txt=getURL("http://www.dataguru.cn/",headerfunction = headers$update)
headers$value()
h = basicHeaderGatherer()
txtt=getURL("http://www.dataguru.cn/",headerfunction = h$update)
names(h$value())
h$value()
myheader <- c(
"User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6) ",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"="en-us",
"Connection"="keep-alive",
"Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)
d = debugGatherer()
d$value()
temp <- getURL("http://www.dataguru.cn/",
httpheader = myheader,
debugfunction=d$update,verbose =TRUE)
cat(d$value()[3])#提交给服务器的头信息
temp <- getURL("http://www.dataguru.cn/",debugfunction=d$update,verbose =TRUE)
cat(d$value()[3])#提交给服务器的头信息
url.exists("http://cos.name/bbs/login.php?")
curl = getCurlHandle()
d=getURL("http://cos.name/cn/topic/411708/", curl = curl)
getCurlInfo(curl)$response.code
getCurlInfo(curl)
h = basicHeaderGatherer()
txtt=getURL("http://cos.name/cn/topic/411708/",headerfunction = h$update)
names(h$value())
h$value()
d$value()
getCurlInfo(curl)
headers = basicTextGatherer()
txt=getURL("http://www.dataguru.cn/",headerfunction = headers$update)
names(headers$value())#说明是字符串形式
headers$value()
wangye = "https://www.baidu.com/s?word=%E6%95%B0%E6%8D%AE%E7%82%BC%E9%87%91&tn=sitehao123&ie=utf-8&ssl_sample=hao_1"
getForm(wangye)
getFormParams(wangye)
wangye = "https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&tn=sitehao123&wd=RCurl&rsv_pq=da9cc0e900002b3e&rsv_t=259epf8EriexZ%2BBZqNgP0OLqTB6cvNAoPESiSn3z8LSnmaQ2jHMfVD76sLy%2F9v03Jg&rsv_enter=1&rsv_sug3=13&rsv_sug1=13&rsv_sug2=0&inputT=7148&rsv_sug4=7149"
getFormParams(wangye)
setwd("E:/workspace/R/常用代码/爬虫")
library(RCurl)
library(XML)
myheader <- c(
"User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6) ",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"="en-us",
"Connection"="keep-alive",
"Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)
temp=getURL("http://t.dianping.com/beijing",
httpheader=myheader,
econding = UTF-8
#           debugfunction=d$update,
#           verbose =TRUE
)
temp=getURL("http://t.dianping.com/beijing",
httpheader=myheader,
econding = "UTF-8"
#           debugfunction=d$update,
#           verbose =TRUE
)
library(RCurl)
library(XML)
myheader <- c(
"User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6) ",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"="en-us",
"Connection"="keep-alive",
"Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)
temp=getURL("http://t.dianping.com/movie/beijing#nav",
httpheader=myheader,
.econding = "UTF-8"
#           debugfunction=d$update,
#           verbose =TRUE
)
rm(temp)
temp=getURL("http://t.dianping.com/movie/beijing#nav",
httpheader=myheader,
econding = "UTF-8"
#           debugfunction=d$update,
#           verbose =TRUE
)
rm(temp)
temp=getURL("http://t.dianping.com/movie/beijing#nav",
httpheader=myheader,
.encoding = "UTF-8"
#           debugfunction=d$update,
#           verbose =TRUE
)
k=htmlParse(temp)
getNodeSet(k,'div[@class="tg-floor-item-wrap"]')
getNodeSet(k,'//div[@class="tg-floor-item-wrap"]')
getNodeSet(k,'//div[@class="tg-floor-item-wrap"]')
k
temp=getURL("http://t.dianping.com/movie/beijing/tab_deal?pageno=1",
httpheader=myheader,
.encoding = "UTF-8"
#           debugfunction=d$update,
#           verbose =TRUE
)
temp
k=htmlParse(temp)
k
getNodeSet(k,'//div [@class="tg-floor-item-wrap"]')
youhui=sapply(getNodeSet(k,'//div[@class="tg-floor-item-wrap"]'),xmlValue)
youhui
youhui = strsplit(youhui, "\n")
youhui
youhui = strsplit(youhui, "\n")[c(5,6,10,13,17)]
lapply(strsplit(youhui, "\n"), fun = function(x) x[c(5,6,10,13,17)])
lapply(strsplit(youhui, "\n"), function(x) x[c(5,6,10,13,17)])
lapply(as.character(strsplit(youhui, "\n")), function(x) x[c(5,6,10,13,17)])
you_str = strsplit(youhui, "\n")
youhui=sapply(getNodeSet(k,'//div[@class="tg-floor-item-wrap"]'),xmlValue)
you_str = strsplit(youhui, "\n")
you_fei = lapply(you_str, function(x) {x[c(5,6,10,13,17)]})
you_fei
grep(" .*?", you_fei)
lapply(you_fei, function(x) {grep(" .*?", x)})
lapply(you_fei, function(x) {grep(" .?", x)})
lapply(you_fei, function(x) {grep("\S", x)})
lapply(you_fei, function(x) {grep("\\S", x)})
zifu = lapply(you_fei, function(x) {strsplit(X, " ")})
zifu = lapply(you_fei, function(x) {strsplit(x, " ")})
zifu
you_fei
lapply(you_fei, function(x) {strsplit(x, "[^\S]")})
lapply(you_fei, function(x) {strsplit(x, "[^\\S]")})
lapply(you_fei, function(x) {strsplit(x, "[^\\s]")})
lapply(you_fei, function(x) {strsplit(x, "[^\\w]")})
strsplit(you_fei, "[^\\w]")
you_fei
youfei[1]
you_fei[1]
dianyingyuan = you_fei[[1]]
dianyingyuan
dianyingyuan = lapply(you_fei, function(x) {x[1]})
dianyingyuan
dianyingyuan = unlist(lapply(you_fei, function(x) {x[1]}))
dianyingyuan
you_fei
unlist(lapply(you_fei, function(x){strsplit(x[2], '，')[1]} ))
unlist(lapply(you_fei, function(x){strsplit(x[2], '，')}[2] ))
lapply(you_fei, function(x){strsplit(x[2], '，')} )
you_fei
lapply(you_fei, function(x) {x[1][1]})
lapply(you_fei, function(x){strsplit(x[2], '，')}[1] )
lapply(you_fei, function(x){strsplit(x[2], '，')[1]} )
lapply(
lapply(you_fei, function(x){strsplit(x[2], '，')} )
function (x) {x[1]}
)
lapply(
lapply(you_fei, function(x){strsplit(x[2], '，')} ),
function (x) {x[1]}
)
unlist(
lapply(you_fei, function(x){strsplit(x[2], '，')} )
)
lapply(you_fei, function(x){strsplit(x[2], '，')} )
a= lapply(you_fei, function(x){strsplit(x[2], '，')} )
lapply(a, function(x) {x[1]})
lapply(a, function(x) {x[[1]]})
lapply(a, function(x) {x[[1]][1]})
lapply(a, function(x) {x[1][1]})
lapply(a, function(x) {x[[1]][1]})
unlist(lapply(you_fei, function(x) {sub('\\S', '', x[3])}))
you_fei
unlist(lapply(you_fei, function(x) {sub('\\s', '', x[3])}))
unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[3])}))
dianyingyuan = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[1])}))
dianyingyuan
xinxi = unlist(lapply(you_fei, function(x){sub('\\s+', '', x[2])} ))
xinxi
shoujia = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[3])}))
#原价
yuanjia = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[4])}))
#已出售票数
piaoshu = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[5])}))
you_fei
data = data.frame(dianyingyuan, shoujia, yuanjia, piaoshu , xinxi)
data
data = data.frame("电影院地址" = dianyingyuan,
"售价" = shoujia,
"原价" = yuanjia,
"已售出票数"=piaoshu ,
"备注" = xinxi)
shuju = file("tuangou.xls", open = "wb")
writeBin(data,shuju )
close(shuju)
write.csv(data,"电影团.csv",  fileEncoding = "UTF-8")
write.csv(data,"电影团.csv",  fileEncoding = "UTF-16LE")
head(data)
data = data.frame("电影院地址" = dianyingyuan,
"售价" = shoujia,
"原价" = yuanjia,
"已售出票数"=piaoshu
#"备注" = xinxi
)
head(data)
write.csv(data,"电影团.csv",  fileEncoding = "UTF-16LE")
write.csv(data,"电影团.csv")
data = data.frame("电影院地址" = dianyingyuan,
"售价" = shoujia,
"原价" = yuanjia,
"已售出票数"=piaoshu,
"备注" = xinxi
)
write.csv(data,"电影团.csv")
tuangou = function(url, i){
myheader <- c(
"User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6) ",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"="en-us",
"Connection"="keep-alive",
"Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)
temp=getURL(url, httpheader=myheader,  .encoding = "UTF-8")
#解析网页
k=htmlParse(temp)
#获得节点
getNodeSet(k,'//div [@class="tg-floor-item-wrap"]')
#处理每个节点
youhui=sapply(getNodeSet(k,'//div[@class="tg-floor-item-wrap"]'),xmlValue)
#分隔节点
you_str = strsplit(youhui, "\n")
#取得有用的数据
you_fei = lapply(you_str, function(x) {x[c(5,6,10,13,17)]})
#电影院
dianyingyuan = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[1])}))
#售价信息
xinxi = unlist(lapply(you_fei, function(x){sub('\\s+', '', x[2])} ))
#折扣价
shoujia = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[3])}))
#原价
yuanjia = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[4])}))
#已出售票数
piaoshu = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[5])}))
data = data.frame("电影院地址" = dianyingyuan,
"售价" = shoujia,
"原价" = yuanjia,
"已售出票数"=piaoshu,
"备注" = xinxi)
dianying = paste("电影团购", i,".csv", sep = "")
write.csv(data, dianying)
}
urllist=0
i=0
page=1:5
urllist[page]=paste("http://t.dianping.com/movie/beijing/tab_deal?pageno=", page , sep='')
urllist
for(url in urllist)
{
i = i+1
tuangou(url, i)
}
url = http://t.dianping.com/movie/beijing/tab_deal?pageno=2
url = http://t.dianping.com/movie/beijing/tab_deal?pageno=2
url = http://t.dianping.com/movie/beijing/tab_deal?pageno=2
#url = ”http://t.dianping.com/movie/beijing/tab_deal?pageno=2“
url = ”http://t.dianping.com/movie/beijing/tab_deal?pageno=2“
url = "http://t.dianping.com/movie/beijing/tab_deal?pageno=2"
temp=getURL(url, httpheader=myheader,  .encoding = "UTF-8")
k=htmlParse(temp)
getNodeSet(k,'//div [@class="tg-floor-item-wrap"]')
youhui=sapply(getNodeSet(k,'//div[@class="tg-floor-item-wrap"]'),xmlValue)
you_str = strsplit(youhui, "\n")
you_fei = lapply(you_str, function(x) {x[c(5,6,10,13,17)]})
dianyingyuan = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[1])}))
#售价信息
xinxi = unlist(lapply(you_fei, function(x){sub('\\s+', '', x[2])} ))
#折扣价
shoujia = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[3])}))
#原价
yuanjia = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[4])}))
#已出售票数
piaoshu = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[5])}))
data = data.frame("电影院地址" = dianyingyuan,
"售价" = shoujia,
"原价" = yuanjia,
"已售出票数"=piaoshu,
"备注" = xinxi)
paste("电影团购", 1,".csv", sep = "")
dianying = paste("电影团购", 1,".csv", sep = "")
write.csv(data, dianying)
tuangou = function(url, i){
myheader <- c(
"User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6) ",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"="en-us",
"Connection"="keep-alive",
"Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)
#url = "http://t.dianping.com/movie/beijing/tab_deal?pageno=2"
temp=getURL(url, httpheader=myheader,  .encoding = "UTF-8")
#解析网页
k=htmlParse(temp)
#获得节点
#getNodeSet(k,'//div [@class="tg-floor-item-wrap"]')
#处理每个节点
youhui=sapply(getNodeSet(k,'//div[@class="tg-floor-item-wrap"]'),xmlValue)
#分隔节点
you_str = strsplit(youhui, "\n")
#取得有用的数据
you_fei = lapply(you_str, function(x) {x[c(5,6,10,13,17)]})
#电影院
dianyingyuan = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[1])}))
#售价信息
xinxi = unlist(lapply(you_fei, function(x){sub('\\s+', '', x[2])} ))
#折扣价
shoujia = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[3])}))
#原价
yuanjia = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[4])}))
#已出售票数
piaoshu = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[5])}))
data = data.frame("电影院地址" = dianyingyuan,
"售价" = shoujia,
"原价" = yuanjia,
"已售出票数"=piaoshu,
"备注" = xinxi)
dianying = paste("电影团购", 1,".csv", sep = "")
write.csv(data, dianying)
}
urllist=0
i=0
page=1:3
urllist[page]=paste("http://t.dianping.com/movie/beijing/tab_deal?pageno=", page , sep='')
for(url in urllist)
{
i = i+1
tuangou(url, i)
}
tuangou = function(url, i){
myheader <- c(
"User-Agent"="Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9.1.6) ",
"Accept"="text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
"Accept-Language"="en-us",
"Connection"="keep-alive",
"Accept-Charset"="GB2312,utf-8;q=0.7,*;q=0.7"
)
#url = "http://t.dianping.com/movie/beijing/tab_deal?pageno=2"
temp=getURL(url, httpheader=myheader,  .encoding = "UTF-8")
#解析网页
k=htmlParse(temp)
#获得节点
#getNodeSet(k,'//div [@class="tg-floor-item-wrap"]')
#处理每个节点
youhui=sapply(getNodeSet(k,'//div[@class="tg-floor-item-wrap"]'),xmlValue)
#分隔节点
you_str = strsplit(youhui, "\n")
#取得有用的数据
you_fei = lapply(you_str, function(x) {x[c(5,6,10,13,17)]})
#电影院
dianyingyuan = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[1])}))
#售价信息
xinxi = unlist(lapply(you_fei, function(x){sub('\\s+', '', x[2])} ))
#折扣价
shoujia = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[3])}))
#原价
yuanjia = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[4])}))
#已出售票数
piaoshu = unlist(lapply(you_fei, function(x) {sub('\\s+', '', x[5])}))
data = data.frame("电影院地址" = dianyingyuan,
"售价" = shoujia,
"原价" = yuanjia,
"已售出票数"=piaoshu,
"备注" = xinxi)
dianying = paste("电影团购", i,".csv", sep = "")
write.csv(data, dianying)
}
urllist=0
i=0
page=1:3
urllist[page]=paste("http://t.dianping.com/movie/beijing/tab_deal?pageno=", page , sep='')
for(url in urllist)
{
i = i+1
tuangou(url, i)
Sys.sleep(2)
}
